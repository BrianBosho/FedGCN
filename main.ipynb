{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5QApBmLhOGP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from models import GCN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.cluster.vq import kmeans,vq\n",
    "#from scipy import stats  \n",
    "\n",
    "#from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "#from itertools import permutations \n",
    "\n",
    "from utils import get_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import generate_data, load_data\n",
    "from train_func import test, train, Block_matrix_train, Lhop_Block_matrix_train, Communicate_train, ADMM_communication_train, Block_matrix_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0WIOFZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    hop_neightbor_index=index\n",
    "    for i in range(K):\n",
    "        hop_neightbor_index=torch.unique(torch.nonzero(adj[hop_neightbor_index])[:,1])\n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyL-gXSZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors_BDS(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    \n",
    "    onehop_neightbor_index=torch.unique(torch.nonzero(adj[index])[:,1])\n",
    "    np.setdiff1d(index, onehop_neightbor_index)\n",
    "    \n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \n",
    "    mx = mx + torch.eye(mx.shape[0],mx.shape[1])\n",
    "    \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return torch.tensor(mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CMAPT6gafH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRjSx8U6gSoo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCrl5TV_eg5t"
   },
   "outputs": [],
   "source": [
    "def centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test):\n",
    "        #choose adj matrix\n",
    "        #GCN:n*n\n",
    "        \n",
    "        #define model\n",
    "\n",
    "        model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout)\n",
    "        if args_cuda:\n",
    "\n",
    "                model=model.to(torch.device('cuda:0'))#.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay)\n",
    "        # Train model\n",
    "        best_val=0\n",
    "        for t in range(iterations): #make to equivalent to federated\n",
    "            loss_train, acc_train=train(t, model, optimizer, features, adj, labels, idx_train)\n",
    "            # validation\n",
    "            loss_train, acc_train= test(model, features, adj, labels, idx_train) #train after backward\n",
    "            print(t,\"train\",loss_train,acc_train)\n",
    "            loss_val, acc_val= test(model, features, adj, labels, idx_val) #validation\n",
    "            print(t,\"val\",loss_val,acc_val)\n",
    "            \n",
    "            a = open(mode+'_'+dataset_name+'_IID_'+'centralized_GCN_iter_'+str(iterations),'a+')\n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            \n",
    "        #test  \n",
    "        loss_test, acc_test= test(model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(mode+'_'+dataset_name+'_IID_'+'centralized_GCN_iter_'+str(iterations),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        \n",
    "        print(\"save file as\",mode+'_'+dataset_name+'_IID_'+'centralized_GCN_iter_'+str(iterations))\n",
    "        return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMHZprmsOTdl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Block_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #GCN:n*n\n",
    "        #no connection between agents\n",
    "\n",
    "        #define model\n",
    "\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(torch.device('cuda:0'))#.cuda()\n",
    "                global_model=global_model.to(torch.device('cuda:0'))\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # split data into K devices\n",
    "        \n",
    "        n=len(adj)\n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append(np.array(labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        L = []\n",
    "        for i in split_data_indexes:\n",
    "            L += list(i)\n",
    "        L.sort()\n",
    "        iid_indexes = np.setdiff1d(range(len(labels)), L)\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "        \n",
    "        \n",
    "        #get train indexes in each device, only part of nodes in each device have labels in the train process\n",
    "        split_train_ids = []\n",
    "        for i in range(K):\n",
    "            split_data_indexes[i].sort()\n",
    "            inter = np.intersect1d(split_data_indexes[i], idx_train)\n",
    "            \n",
    "            split_train_ids.append(np.searchsorted(split_data_indexes[i], inter))   #local id in block matrix\n",
    "            \n",
    "        \n",
    "        \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        \n",
    "        #start training\n",
    "        for t in range(iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(split_train_ids[i]) == 0:\n",
    "                        continue\n",
    "                    acc_train=Block_matrix_train(epoch, models[i], optimizers[i], features, adj, labels,\n",
    "                                    split_data_indexes[i], split_train_ids[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "                    #print(model.Lambda)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            \n",
    "            \n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = split_train_ids[0].shape[0] * states[0][key]\n",
    "                count_D=split_train_ids[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += split_train_ids[i].shape[0] * states[i][key]\n",
    "                    count_D += split_train_ids[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "            \n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            #print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            #print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        #print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        #print(\"save file as\",mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "\n",
    "\n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDS_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop = 1, sample_rate = 0.5): \n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout)\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(torch.device('cuda:0'))#.cuda()\n",
    "                global_model=global_model.to(torch.device('cuda:0'))\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        n=len(adj)\n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append(np.array(labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        L = []\n",
    "        for i in split_data_indexes:\n",
    "            L += list(i)\n",
    "        L.sort()\n",
    "        iid_indexes = np.setdiff1d(range(len(labels)), L)\n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            \n",
    "            split_data_indexes[i].sort()\n",
    "            \n",
    "            communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i].sort()\n",
    "\n",
    "            inter = np.intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "            \n",
    "            in_com_train_data_indexes.append(np.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for t in range(iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    inter = np.intersect1d(split_data_indexes[i], communicate_indexes[i])\n",
    "                    \n",
    "                    \n",
    "                    sample_indexs = np.concatenate((split_data_indexes[i], np.random.choice(inter, int(len(inter) * sample_rate))), axis=0)\n",
    "                    \n",
    "                    acc_train = Lhop_Block_matrix_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, sample_indexs, in_com_train_data_indexes[i])\n",
    "                    ######### add decay learning rate\n",
    "                    \n",
    "                    #for g in optimizers[i].param_groups:\n",
    "                        #g['lr'] = args_lr / (t + epoch + 1)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "            \n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "\n",
    "        return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq3EUy_UPqz4"
   },
   "outputs": [],
   "source": [
    "def Lhop_Block_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop): #no information loss\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout)\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(torch.device('cuda:0'))#.cuda()\n",
    "                global_model=global_model.to(torch.device('cuda:0'))\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        n=len(adj)\n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append(np.array(labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        L = []\n",
    "        for i in split_data_indexes:\n",
    "            L += list(i)\n",
    "        L.sort()\n",
    "        iid_indexes = np.setdiff1d(range(len(labels)), L)\n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            \n",
    "            split_data_indexes[i].sort()\n",
    "            \n",
    "            communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i].sort()\n",
    "\n",
    "            inter = np.intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "            \n",
    "            in_com_train_data_indexes.append(np.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for t in range(iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                    acc_train = Lhop_Block_matrix_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, communicate_indexes[i], in_com_train_data_indexes[i])\n",
    "                    ######### add decay learning rate\n",
    "                    \n",
    "                    #for g in optimizers[i].param_groups:\n",
    "                        #g['lr'] = args_lr / (t + epoch + 1)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "            \n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_GCN_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "\n",
    "        return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubM8c3SqXwA3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "mode=\"real\"\n",
    "\n",
    "if mode=='simulate':\n",
    "    dataset_name=''\n",
    "    \n",
    "    number_of_nodes=200\n",
    "    class_num=3\n",
    "    link_inclass_prob=10/number_of_nodes  #when calculation , remove the link in itself\n",
    "    #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
    "\n",
    "    link_outclass_prob=link_inclass_prob/20\n",
    "\n",
    "\n",
    "    features, adj, labels, idx_train, idx_val, idx_test =generate_data(number_of_nodes,  class_num, link_inclass_prob, link_outclass_prob)               \n",
    "elif mode=='real':\n",
    "    #'cora', 'citeseer', 'pubmed' #other dataset twitter, \n",
    "    dataset_name='cora'\n",
    "    \n",
    "    features, adj, labels, idx_train, idx_val, idx_test = load_data(dataset_name)\n",
    "    class_num = labels.max().item() + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jDXPAhGUu16",
    "outputId": "2bbdd38b-f48d-4338-dd07-41bcce391781",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for fix seed, need to rerun both data and model codes\n",
    "\n",
    "args_normalize = True\n",
    "\n",
    "model_type = 'GCN'    #GCN\n",
    "args_hidden = 16\n",
    "args_dropout = 0.5\n",
    "args_lr = 0.5\n",
    "args_weight_decay = 5e-4     #L2 penalty\n",
    "args_epochs = 3\n",
    "args_no_cuda = False\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "args_device_num = class_num #split data into args_device_num parts\n",
    "iterations = 300\n",
    "\n",
    "\n",
    "\n",
    "if args_normalize==True:  \n",
    "    adj = normalize(adj)\n",
    "    '''\n",
    "    adj = adj + torch.eye(adj.shape[0],adj.shape[1])\n",
    "    d=torch.sum(adj,axis=1)\n",
    "    D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "    D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
    "    adj = torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test)\n",
    "    \n",
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for i in range(10):\n",
    "            Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign)\n",
    "            BDS_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 1)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
