{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5QApBmLhOGP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from models import GCN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from utils import get_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import generate_data, load_data\n",
    "from train_func import test, train, Block_matrix_train, Lhop_Block_matrix_train, Communicate_train, ADMM_communication_train, Block_matrix_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0WIOFZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    hop_neightbor_index=index\n",
    "    for i in range(K):\n",
    "        hop_neightbor_index=torch.unique(torch.nonzero(adj[hop_neightbor_index])[:,1]) #include 1-k hop neightbors\n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_two_layer(features, adj_2_hop, adj_1_hop, number_of_datapoints):\n",
    "    #adj_2_hop = normalize(adj_2_hop)\n",
    "    #adj_1_hop = normalize(adj_1_hop)\n",
    "    return torch.norm(features.T.mm(adj_2_hop.T).mm(adj_1_hop.T).mm(adj_1_hop).mm(adj_2_hop).mm(features)) / number_of_datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyL-gXSZPqz2"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \n",
    "    mx = mx + torch.eye(mx.shape[0],mx.shape[1])\n",
    "    \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return torch.tensor(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jDXPAhGUu16",
    "outputId": "2bbdd38b-f48d-4338-dd07-41bcce391781",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_graph(dataset_name, iid_percent, K_over_class_num):\n",
    "    \n",
    "        if dataset_name=='simulate':\n",
    "            \n",
    "\n",
    "            number_of_nodes=200\n",
    "            class_num=3\n",
    "            link_inclass_prob=10/number_of_nodes  #when calculation , remove the link in itself\n",
    "            #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
    "            link_outclass_prob=link_inclass_prob/20\n",
    "            features, adj, labels, idx_train, idx_val, idx_test =generate_data(number_of_nodes,  class_num, link_inclass_prob, link_outclass_prob)               \n",
    "        else:\n",
    "            #'cora', 'citeseer', 'pubmed' #other dataset twitter, \n",
    "            \n",
    "\n",
    "            features, adj, labels, idx_train, idx_val, idx_test = load_data(dataset_name)\n",
    "            class_num = labels.max().item() + 1\n",
    "\n",
    "\n",
    "        #client num\n",
    "        K = K_over_class_num * class_num\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append(np.array(labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        L = []\n",
    "        for i in split_data_indexes:\n",
    "            L += list(i)\n",
    "        L.sort()\n",
    "        iid_indexes = np.setdiff1d(range(len(labels)), L)\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "        \n",
    "        \n",
    "        #get train indexes in each device, only part of nodes in each device have labels in the train process\n",
    "        split_train_ids = []\n",
    "        for i in range(K):\n",
    "            split_data_indexes[i].sort()\n",
    "            inter = np.intersect1d(split_data_indexes[i], idx_train)\n",
    "            split_train_ids.append(np.searchsorted(split_data_indexes[i], inter))   #local id in block matrix\n",
    "\n",
    "        one_hot_labels = F.one_hot(labels).float()\n",
    "        \n",
    "        return features, adj, labels, one_hot_labels, split_data_indexes, K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communication Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "dataset_name='simulate'\n",
    "args_normalize = True\n",
    "\n",
    "\n",
    "for percent in range(0, 11, 1):\n",
    "    length_current_index = []\n",
    "    length_neighbor1 = []\n",
    "    length_neighbor2 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        iid_percent = percent / 10\n",
    "        K_over_class_num = 1\n",
    "        #node_num = 100\n",
    "        Class_num = 3\n",
    "        \n",
    "        features, adj, labels, one_hot_labels, split_data_indexes, K = get_graph(dataset_name, iid_percent, K_over_class_num)\n",
    "\n",
    "\n",
    "        \n",
    "        L = []\n",
    "        for i in range(Class_num-1):\n",
    "            L+= [i] * int(node_num / Class_num)\n",
    "        L += [Class_num-1] * (node_num - len(L))\n",
    "        \n",
    "        Theta = F.one_hot(torch.tensor(L)).float()\n",
    "        #L = [1 / Class_num] * Class_num\n",
    "        #Theta = torch.tensor([L for i in range(node_num)]).float()\n",
    "\n",
    "        #Lambda = torch.eye(Class_num)\n",
    "        A = torch.eye(Class_num) * link_inclass_prob\n",
    "\n",
    "        B = link_outclass_prob * (torch.ones((Class_num, Class_num)) - torch.eye(Class_num))\n",
    "\n",
    "        Lambda = A+B\n",
    "\n",
    "\n",
    "        adj = Theta.mm(Lambda).mm(Theta.T)\n",
    "        features = torch.eye(len(adj))\n",
    "        one_hot_labels = Theta \n",
    "        adj = normalize(adj)\n",
    "        get_gradient_two_layer(features, adj_2_hop, adj_1_hop, number_of_datapoints)\n",
    "        \n",
    "        non_iid_num = int(node_num / Class_num * p)\n",
    "\n",
    "        Local_index = list(range(int(node_num / Class_num * p))) + list(np.random.randint(int(node_num / Class_num) + 1,node_num, int(node_num / Class_num * (1-p))))\n",
    "        print(Local_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "dataset_name='simulate'\n",
    "args_normalize = True\n",
    "\n",
    "List_length_current_index = []\n",
    "List_length_neighbor1 = []\n",
    "List_length_neighbor2 = []\n",
    "\n",
    "\n",
    "for percent in range(0, 11, 1):\n",
    "    length_current_index = []\n",
    "    length_neighbor1 = []\n",
    "    length_neighbor2 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        iid_percent = percent / 10\n",
    "        K_over_class_num = 1\n",
    "        features, adj, labels, one_hot_labels, split_data_indexes, K = get_graph(dataset_name, iid_percent, K_over_class_num)\n",
    "        if dataset_name=='simulate':\n",
    "                feature = one_hot_labels\n",
    "        if args_normalize:\n",
    "            adj = normalize(adj)\n",
    "\n",
    "        for i in range(len(split_data_indexes)):\n",
    "                current_index = split_data_indexes[i]\n",
    "\n",
    "                neighbor1 = get_K_hop_neighbors(adj, current_index, 1)\n",
    "\n",
    "                neighbor2 = get_K_hop_neighbors(adj, current_index, 2)\n",
    "\n",
    "                #print(len(current_index), len(neighbor1), len(neighbor2))\n",
    "                length_current_index.append(len(current_index))\n",
    "                length_neighbor1.append(len(neighbor1))\n",
    "                length_neighbor2.append(len(neighbor2))\n",
    "        print(np.array(length_current_index).sum() / len(length_current_index))\n",
    "        print(np.array(length_neighbor1).sum() / len(length_neighbor1))\n",
    "        print(np.array(length_neighbor2).sum() / len(length_neighbor2))\n",
    "    \n",
    "    List_length_current_index.append(np.array(length_current_index).sum() / len(length_current_index))\n",
    "    List_length_neighbor1.append(np.array(length_neighbor1).sum() / len(length_neighbor1))\n",
    "    List_length_neighbor2.append(np.array(length_neighbor2).sum() / len(length_neighbor2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "X = np.array(range(0, 11, 1)) / 10\n",
    "plt.plot(X, [0]*len(X), 's-', label = 'FedGCN(0-hop)', markersize=8)\n",
    "plt.plot(X, L, '+-', label = 'FedGCN(1-hop)', markersize=8)\n",
    "plt.plot(X, np.array(List_length_neighbor1) - np.array(List_length_current_index), '*-', label = 'FedGCN(2-hop)', markersize=8)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('IID Degree', fontsize=25)\n",
    "plt.ylabel('Communication Cost', fontsize=25)\n",
    "plt.legend(fontsize=20, frameon=True, loc = 'best')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.savefig(\"simulate_communication_cost_changeIID.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "X = np.array(range(0, 11, 1)) / 10\n",
    "plt.plot(X, List_length_current_index, 's-', label = '0-hop', markersize=8)\n",
    "plt.plot(X, List_length_neighbor1, '+-', label = '1-hop', markersize=8)\n",
    "plt.plot(X, List_length_neighbor2, '*-', label = '2-hop', markersize=8)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('IID Degree', fontsize=25)\n",
    "plt.ylabel('Number of Nodes', fontsize=25)\n",
    "plt.legend(fontsize=20, frameon=True, loc = 'best')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.savefig(\"simulate_node_number_changeIID.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "dataset_name='simulate'\n",
    "args_normalize = True\n",
    "Y_Norm_Local_0hop_Global = []\n",
    "Y_Norm_Local_1hop_Global = []\n",
    "Y_Norm_Local_2hop_Global = []\n",
    "\n",
    "for percent in range(0, 11, 1):\n",
    "    \n",
    "    iid_percent = percent / 10\n",
    "    K_over_class_num = 1\n",
    "    features, adj, labels, one_hot_labels, split_data_indexes, K = get_graph(dataset_name, iid_percent, K_over_class_num)\n",
    "    #if dataset_name=='simulate':\n",
    "    #        feature = one_hot_labels\n",
    "    if args_normalize:\n",
    "        adj = normalize(adj)\n",
    "    Norm_Local_0hop_Global = []\n",
    "    Norm_Local_1hop_Global = []\n",
    "    Norm_Local_2hop_Global = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(split_data_indexes)):\n",
    "            current_index = split_data_indexes[i]\n",
    "            Local_0hop = get_gradient_two_layer(features[current_index], adj[current_index][:,current_index], adj[current_index][:,current_index], len(current_index))\n",
    "\n",
    "            neighbor1 = get_K_hop_neighbors(adj, current_index, 1)\n",
    "            Local_1hop = get_gradient_two_layer(features[neighbor1], adj[current_index][:,neighbor1], adj[current_index][:,current_index], len(current_index))\n",
    " \n",
    "            neighbor2 = get_K_hop_neighbors(adj, current_index, 2)\n",
    "\n",
    "            Local_2hop = get_gradient_two_layer(features[neighbor2], adj[neighbor1][:,neighbor2], adj[current_index][:,neighbor1], len(current_index))\n",
    "            #Local_comm = get_gradient_two_layer(features[neighbor2], adj[neighbor2][:,neighbor2], adj[neighbor2][:,neighbor2], len(neighbor2))\n",
    "\n",
    "            Global = get_gradient_two_layer(features, adj, adj, len(adj))\n",
    "\n",
    "            #print(\"Local_0hop\", float(torch.norm(Local_0hop)))\n",
    "            #print(\"Local_1hop\", float(torch.norm(Local_1hop)))\n",
    "            #print(\"Local_2hop\", float(torch.norm(Local_2hop)))\n",
    "            #print(\"Global\", float(torch.norm(Global)))\n",
    "\n",
    "            #print(\"Norm Local_comm Local_no_comm\", float(torch.norm(Local_comm - Local_no_comm)))\n",
    "            \n",
    "            \n",
    "            Norm_Local_0hop_Global.append(float(torch.norm(Local_0hop - Global)))\n",
    "            Norm_Local_1hop_Global.append(float(torch.norm(Local_1hop - Global)))\n",
    "            Norm_Local_2hop_Global.append(float(torch.norm(Local_2hop - Global)))\n",
    "    print(\"Norm Local_0hop Global\", np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    print(\"Norm Local_1hop Global\", np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    print(\"Norm Local_2hop Global\", np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "    Y_Norm_Local_0hop_Global.append(np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    Y_Norm_Local_1hop_Global.append(np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    Y_Norm_Local_2hop_Global.append(np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "            \n",
    "            \n",
    "np.save(dataset_name + \"_Norm Local_0hop Global.npy\", Y_Norm_Local_0hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_1hop Global.npy\", Y_Norm_Local_1hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_2hop Global.npy\", Y_Norm_Local_2hop_Global)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array(range(0,11,1)) / 10\n",
    "\n",
    "plt.plot(X, Y_Norm_Local_0hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_1hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_2hop_Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "dataset_name = 'cora'\n",
    "Y_Norm_Local_0hop_Global = np.load(dataset_name + \"_Norm Local_0hop Global.npy\")\n",
    "Y_Norm_Local_1hop_Global = np.load(dataset_name + \"_Norm Local_1hop Global.npy\")\n",
    "Y_Norm_Local_2hop_Global = np.load(dataset_name + \"_Norm Local_2hop Global.npy\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array(range(0,11,1)) / 10\n",
    "\n",
    "plt.plot(X, Y_Norm_Local_0hop_Global, 's-', label = 'FedGCN(0-hop)', markersize=8)\n",
    "plt.plot(X, Y_Norm_Local_1hop_Global, '+-', label = 'FedGCN(1-hop)', markersize=8)\n",
    "plt.plot(X, Y_Norm_Local_2hop_Global, '*-', label = 'FedGCN(2-hop)', markersize=8)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.legend(fontsize=20, frameon=True, loc = 'best')\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('IID Degree', fontsize=25)\n",
    "plt.ylabel(r'$\\mathit{\\|\\|\\nabla {F_k}(\\mathbf{w_k}) - \\nabla {f}(\\mathbf{w})\\|\\|}$', fontsize=25)\n",
    "plt.legend(fontsize=20, frameon=True, loc = 'best')\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.savefig(\"cora_norm_data_distribution.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "dataset_name='citeseer'\n",
    "args_normalize = True\n",
    "Y_Norm_Local_0hop_Global = []\n",
    "Y_Norm_Local_1hop_Global = []\n",
    "Y_Norm_Local_2hop_Global = []\n",
    "\n",
    "for percent in range(0, 11, 1):\n",
    "    \n",
    "    iid_percent = percent / 10\n",
    "    K_over_class_num = 1\n",
    "    features, adj, labels, one_hot_labels, split_data_indexes, K = get_graph(dataset_name, iid_percent, K_over_class_num)\n",
    "    if dataset_name=='simulate':\n",
    "            feature = one_hot_labels\n",
    "    if args_normalize:\n",
    "        adj = normalize(adj)\n",
    "    Norm_Local_0hop_Global = []\n",
    "    Norm_Local_1hop_Global = []\n",
    "    Norm_Local_2hop_Global = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(split_data_indexes)):\n",
    "            current_index = split_data_indexes[i]\n",
    "            Local_0hop = get_gradient_two_layer(features[current_index], adj[current_index][:,current_index], adj[current_index][:,current_index], len(current_index))\n",
    "\n",
    "            neighbor1 = get_K_hop_neighbors(adj, current_index, 1)\n",
    "            Local_1hop = get_gradient_two_layer(features[neighbor1], adj[current_index][:,neighbor1], adj[current_index][:,current_index], len(current_index))\n",
    " \n",
    "            neighbor2 = get_K_hop_neighbors(adj, current_index, 2)\n",
    "\n",
    "            Local_2hop = get_gradient_two_layer(features[neighbor2], adj[neighbor1][:,neighbor2], adj[current_index][:,neighbor1], len(current_index))\n",
    "            #Local_comm = get_gradient_two_layer(features[neighbor2], adj[neighbor2][:,neighbor2], adj[neighbor2][:,neighbor2], len(neighbor2))\n",
    "\n",
    "            Global = get_gradient_two_layer(features, adj, adj, len(adj))\n",
    "\n",
    "            #print(\"Local_0hop\", float(torch.norm(Local_0hop)))\n",
    "            #print(\"Local_1hop\", float(torch.norm(Local_1hop)))\n",
    "            #print(\"Local_2hop\", float(torch.norm(Local_2hop)))\n",
    "            #print(\"Global\", float(torch.norm(Global)))\n",
    "\n",
    "            #print(\"Norm Local_comm Local_no_comm\", float(torch.norm(Local_comm - Local_no_comm)))\n",
    "            \n",
    "            \n",
    "            Norm_Local_0hop_Global.append(float(torch.norm(Local_0hop - Global)))\n",
    "            Norm_Local_1hop_Global.append(float(torch.norm(Local_1hop - Global)))\n",
    "            Norm_Local_2hop_Global.append(float(torch.norm(Local_2hop - Global)))\n",
    "    print(\"Norm Local_0hop Global\", np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    print(\"Norm Local_1hop Global\", np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    print(\"Norm Local_2hop Global\", np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "    Y_Norm_Local_0hop_Global.append(np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    Y_Norm_Local_1hop_Global.append(np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    Y_Norm_Local_2hop_Global.append(np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "            \n",
    "            \n",
    "np.save(dataset_name + \"_Norm Local_0hop Global.npy\", Y_Norm_Local_0hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_1hop Global.npy\", Y_Norm_Local_1hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_2hop Global.npy\", Y_Norm_Local_2hop_Global)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array(range(0,11,1)) / 10\n",
    "\n",
    "plt.plot(X, Y_Norm_Local_0hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_1hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_2hop_Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "dataset_name='pubmed'\n",
    "args_normalize = True\n",
    "Y_Norm_Local_0hop_Global = []\n",
    "Y_Norm_Local_1hop_Global = []\n",
    "Y_Norm_Local_2hop_Global = []\n",
    "\n",
    "for percent in range(0, 11, 1):\n",
    "    \n",
    "    iid_percent = percent / 10\n",
    "    K_over_class_num = 1\n",
    "    features, adj, labels, one_hot_labels, split_data_indexes, K = get_graph(dataset_name, iid_percent, K_over_class_num)\n",
    "    if dataset_name=='simulate':\n",
    "            feature = one_hot_labels\n",
    "    if args_normalize:\n",
    "        adj = normalize(adj)\n",
    "    Norm_Local_0hop_Global = []\n",
    "    Norm_Local_1hop_Global = []\n",
    "    Norm_Local_2hop_Global = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(split_data_indexes)):\n",
    "            current_index = split_data_indexes[i]\n",
    "            Local_0hop = get_gradient_two_layer(features[current_index], adj[current_index][:,current_index], adj[current_index][:,current_index], len(current_index))\n",
    "\n",
    "            neighbor1 = get_K_hop_neighbors(adj, current_index, 1)\n",
    "            Local_1hop = get_gradient_two_layer(features[neighbor1], adj[current_index][:,neighbor1], adj[current_index][:,current_index], len(current_index))\n",
    " \n",
    "            neighbor2 = get_K_hop_neighbors(adj, current_index, 2)\n",
    "\n",
    "            Local_2hop = get_gradient_two_layer(features[neighbor2], adj[neighbor1][:,neighbor2], adj[current_index][:,neighbor1], len(current_index))\n",
    "            #Local_comm = get_gradient_two_layer(features[neighbor2], adj[neighbor2][:,neighbor2], adj[neighbor2][:,neighbor2], len(neighbor2))\n",
    "\n",
    "            Global = get_gradient_two_layer(features, adj, adj, len(adj))\n",
    "\n",
    "            #print(\"Local_0hop\", float(torch.norm(Local_0hop)))\n",
    "            #print(\"Local_1hop\", float(torch.norm(Local_1hop)))\n",
    "            #print(\"Local_2hop\", float(torch.norm(Local_2hop)))\n",
    "            #print(\"Global\", float(torch.norm(Global)))\n",
    "\n",
    "            #print(\"Norm Local_comm Local_no_comm\", float(torch.norm(Local_comm - Local_no_comm)))\n",
    "            \n",
    "            \n",
    "            Norm_Local_0hop_Global.append(float(torch.norm(Local_0hop - Global)))\n",
    "            Norm_Local_1hop_Global.append(float(torch.norm(Local_1hop - Global)))\n",
    "            Norm_Local_2hop_Global.append(float(torch.norm(Local_2hop - Global)))\n",
    "    print(\"Norm Local_0hop Global\", np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    print(\"Norm Local_1hop Global\", np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    print(\"Norm Local_2hop Global\", np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "    Y_Norm_Local_0hop_Global.append(np.array(Norm_Local_0hop_Global).sum() / len(Norm_Local_0hop_Global))\n",
    "    Y_Norm_Local_1hop_Global.append(np.array(Norm_Local_1hop_Global).sum() / len(Norm_Local_1hop_Global))\n",
    "    Y_Norm_Local_2hop_Global.append(np.array(Norm_Local_2hop_Global).sum() / len(Norm_Local_2hop_Global))\n",
    "            \n",
    "            \n",
    "np.save(dataset_name + \"_Norm Local_0hop Global.npy\", Y_Norm_Local_0hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_1hop Global.npy\", Y_Norm_Local_1hop_Global)\n",
    "np.save(dataset_name + \"_Norm Local_2hop Global.npy\", Y_Norm_Local_2hop_Global)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array(range(0,11,1)) / 10\n",
    "\n",
    "plt.plot(X, Y_Norm_Local_0hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_1hop_Global)\n",
    "plt.plot(X, Y_Norm_Local_2hop_Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
